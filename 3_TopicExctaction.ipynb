{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En este notebook finalmente probaré si un Topic extraction no supervisado funciona mejor que los anteriores modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_md', disable = ['parser', 'ner'])\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "def lemmatize_doc(text, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'PROPN']):\n",
    "    text_out = [t.lemma_.lower() for t in nlp(text)\n",
    "                if t.pos_ in allowed_postags\n",
    "                and len(t.lemma_)>3\n",
    "                and not t.is_stop]\n",
    "    return text_out\n",
    "\n",
    "def build_texts(fname):\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            yield lemmatize_doc(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lee_data_file = 'sem_eval_train_es_topic.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-Es-01643,\"@aliciaenp Ajajjaa somos del clan twitteras perdidas pa eventos \"\"importantes\"\" \"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(lee_data_file) as f:\n",
    "    for line in f:\n",
    "        print(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = build_texts(lee_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_procesado = [c for c in texto]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3561"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lista_procesado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]\n"
     ]
    }
   ],
   "source": [
    "diccionario = corpora.Dictionary(build_texts(lee_data_file))\n",
    "corpus = [diccionario.doc2bow(text) for text in build_texts(lee_data_file)]\n",
    "\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7509"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(diccionario.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.017*\"persona\" + 0.011*\"amar\" + 0.009*\"sentir\" + 0.008*\"querer\" + '\n",
      "  '0.007*\"miedo\" + 0.007*\"café\" + 0.007*\"mujer\" + 0.007*\"amigo\" + '\n",
      "  '0.007*\"conocer\" + 0.006*\"depresión\"'),\n",
      " (1,\n",
      "  '0.010*\"llamar\" + 0.009*\"amigo\" + 0.009*\"amar\" + 0.007*\"mundo\" + '\n",
      "  '0.007*\"muerte\" + 0.007*\"depresión\" + 0.006*\"cosa\" + 0.006*\"escuchar\" + '\n",
      "  '0.006*\"palabra\" + 0.006*\"despertar\"'),\n",
      " (2,\n",
      "  '0.013*\"enojo\" + 0.013*\"gente\" + 0.012*\"dejar\" + 0.011*\"risa\" + '\n",
      "  '0.009*\"feliz\" + 0.008*\"triste\" + 0.007*\"volver\" + 0.006*\"hablar\" + '\n",
      "  '0.006*\"tomar\" + 0.005*\"dolor\"'),\n",
      " (3,\n",
      "  '0.013*\"lamentable\" + 0.010*\"insulto\" + 0.008*\"noche\" + 0.008*\"jajaja\" + '\n",
      "  '0.007*\"familia\" + 0.007*\"infeliz\" + 0.007*\"dejar\" + 0.006*\"enfadado\" + '\n",
      "  '0.006*\"pesadilla\" + 0.006*\"terrible\"'),\n",
      " (4,\n",
      "  '0.014*\"gustar\" + 0.013*\"querer\" + 0.011*\"poner\" + 0.011*\"esperar\" + '\n",
      "  '0.010*\"dormir\" + 0.010*\"cansado\" + 0.010*\"amigo\" + 0.008*\"lindo\" + '\n",
      "  '0.008*\"andar\" + 0.007*\"cabeza\"'),\n",
      " (5,\n",
      "  '0.011*\"sacar\" + 0.011*\"querer\" + 0.008*\"depender\" + 0.008*\"grave\" + '\n",
      "  '0.007*\"hora\" + 0.007*\"gente\" + 0.007*\"felicidad\" + 0.006*\"sentir\" + '\n",
      "  '0.006*\"momento\" + 0.006*\"pasar\"'),\n",
      " (6,\n",
      "  '0.015*\"pasar\" + 0.013*\"llorar\" + 0.013*\"ironía\" + 0.011*\"feliz\" + '\n",
      "  '0.008*\"pena\" + 0.007*\"querer\" + 0.007*\"encantar\" + 0.007*\"vida\" + '\n",
      "  '0.007*\"valer\" + 0.007*\"mano\"'),\n",
      " (7,\n",
      "  '0.015*\"mañana\" + 0.014*\"dormir\" + 0.008*\"leer\" + 0.007*\"sueño\" + '\n",
      "  '0.007*\"clase\" + 0.007*\"viejo\" + 0.007*\"sentir\" + 0.006*\"horror\" + '\n",
      "  '0.006*\"genial\" + 0.006*\"perdón\"'),\n",
      " (8,\n",
      "  '0.017*\"querer\" + 0.011*\"odio\" + 0.009*\"salir\" + 0.008*\"negro\" + '\n",
      "  '0.007*\"agradable\" + 0.007*\"casa\" + 0.007*\"cosa\" + 0.007*\"deprimido\" + '\n",
      "  '0.007*\"enojado\" + 0.007*\"ofender\"'),\n",
      " (9,\n",
      "  '0.010*\"enogir\" + 0.009*\"terrible\" + 0.008*\"querer\" + 0.008*\"alarma\" + '\n",
      "  '0.008*\"gracioso\" + 0.007*\"amargo\" + 0.007*\"vida\" + 0.007*\"aburrido\" + '\n",
      "  '0.007*\"amiga\" + 0.007*\"temor\"'),\n",
      " (10,\n",
      "  '0.012*\"perder\" + 0.011*\"cosa\" + 0.009*\"poner\" + 0.009*\"pasar\" + '\n",
      "  '0.008*\"triste\" + 0.008*\"llorar\" + 0.006*\"rabia\" + 0.006*\"tristeza\" + '\n",
      "  '0.006*\"susto\" + 0.006*\"rápido\"')]\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ldamodel = LdaModel(corpus = corpus, num_topics=11, id2word=diccionario, iterations=5000)\n",
    "pprint(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.012996079),\n",
       " (1, 0.012995977),\n",
       " (2, 0.012996083),\n",
       " (3, 0.012995977),\n",
       " (4, 0.8700388),\n",
       " (5, 0.012995977),\n",
       " (6, 0.012995977),\n",
       " (7, 0.012996076),\n",
       " (8, 0.012995977),\n",
       " (9, 0.01299635),\n",
       " (10, 0.012996721)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dist_per_tweet = [ldamodel[corpus[i]] for i in range(len(corpus))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las predicciones del modelo LDA son difíciles de relacionar con las clasificaciones del dataframe. Por lo que dejo la siguiente estrategia para implementarla en caso de tener tiempo suficiente:\n",
    "- Crear una crosstab para ver qué clase predicha se relaciona con cada topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">No veo interesante seguir con este notebook debido a:</span>\n",
    "\n",
    "- La clasificación es multiclase y multietiqueta, por lo que no basta con decir el máximo, sino que habría que decidir un thresshold para decidir cuando un tweet entra en una categoría y cuando no.\n",
    "- He revisado algunos tweets y no tienen ninguna relación las predicciones con las etiquetas reales. Hay varios tweet predichos como una clase que pertenecen a clases completamente diferentes.\n",
    "\n",
    "Pese a esto, si que parece interesante el enfoque ya que pone en la misma clase palabras como pesadilla, lamentable infeliz... En otra clase persona, querer, amar, sentir...\n",
    "\n",
    "Por lo que parece ser que no es una clasificación aleatoria, aunque creo que otros métodos supervisados serán más precisos.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trabajonlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
