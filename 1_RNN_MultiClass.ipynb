{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajo final módulo NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clasificación multietiqueta de los tweets del archivo sem_eval_train.csv respecto a 11 sentimientos:\n",
    "- anger\n",
    "- anticipation\n",
    "- disgust\n",
    "- fear\n",
    "- joy\n",
    "- love\n",
    "- optimism\n",
    "- pessimism\n",
    "- sadness\n",
    "- surprise\n",
    "- trust\n",
    "\n",
    "#### Cada tweet puede pertenecer a varias clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En este notebook probaré un modelo sencillo usando una RNN para un clasificador Multi clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, string, spacy\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, SpatialDropout1D, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_DIM = 50\n",
    "RNN_layer = LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pessimism</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-Es-01643</th>\n",
       "      <td>@aliciaenp Ajajjaa somos del clan twitteras pe...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-Es-05142</th>\n",
       "      <td>@AwadaNai la mala suerte del gato fichame la c...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-Es-05379</th>\n",
       "      <td>@audiomano A mí tampoco me agrado mucho eso. E...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-Es-00208</th>\n",
       "      <td>Para llevar a los bebes de un lugar a otro deb...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-Es-01385</th>\n",
       "      <td>@DalasReview me encanta la terrible hipocresia...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-Es-06340</th>\n",
       "      <td>Ahorita quisiera que mi preocupación más grand...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-Es-00439</th>\n",
       "      <td>El mayor criminal del país diciéndole “delincu...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-Es-04919</th>\n",
       "      <td>Mi prima de 4 años se ha enfadado conmigo porq...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-Es-02703</th>\n",
       "      <td>@lennycia Jajaja...  Ya seee</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-Es-02680</th>\n",
       "      <td>Quiero abrazar. Quiero querer. Me hace falta e...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3561 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           Tweet  anger  \\\n",
       "ID                                                                        \n",
       "2018-Es-01643  @aliciaenp Ajajjaa somos del clan twitteras pe...  False   \n",
       "2018-Es-05142  @AwadaNai la mala suerte del gato fichame la c...  False   \n",
       "2018-Es-05379  @audiomano A mí tampoco me agrado mucho eso. E...   True   \n",
       "2018-Es-00208  Para llevar a los bebes de un lugar a otro deb...  False   \n",
       "2018-Es-01385  @DalasReview me encanta la terrible hipocresia...   True   \n",
       "...                                                          ...    ...   \n",
       "2018-Es-06340  Ahorita quisiera que mi preocupación más grand...  False   \n",
       "2018-Es-00439  El mayor criminal del país diciéndole “delincu...   True   \n",
       "2018-Es-04919  Mi prima de 4 años se ha enfadado conmigo porq...   True   \n",
       "2018-Es-02703                      @lennycia Jajaja...  Ya seee   False   \n",
       "2018-Es-02680  Quiero abrazar. Quiero querer. Me hace falta e...  False   \n",
       "\n",
       "               anticipation  disgust   fear    joy   love  optimism  \\\n",
       "ID                                                                    \n",
       "2018-Es-01643         False    False  False   True  False     False   \n",
       "2018-Es-05142         False    False   True  False  False     False   \n",
       "2018-Es-05379         False    False  False  False  False     False   \n",
       "2018-Es-00208         False    False  False   True  False     False   \n",
       "2018-Es-01385         False     True  False  False  False     False   \n",
       "...                     ...      ...    ...    ...    ...       ...   \n",
       "2018-Es-06340         False    False   True  False  False     False   \n",
       "2018-Es-00439         False     True  False  False  False     False   \n",
       "2018-Es-04919         False    False  False  False  False     False   \n",
       "2018-Es-02703         False    False  False   True  False     False   \n",
       "2018-Es-02680         False    False  False  False   True     False   \n",
       "\n",
       "               pessimism  sadness  surprise  trust  \n",
       "ID                                                  \n",
       "2018-Es-01643      False    False     False  False  \n",
       "2018-Es-05142       True    False     False  False  \n",
       "2018-Es-05379      False    False     False  False  \n",
       "2018-Es-00208      False    False     False  False  \n",
       "2018-Es-01385      False    False     False  False  \n",
       "...                  ...      ...       ...    ...  \n",
       "2018-Es-06340       True     True     False  False  \n",
       "2018-Es-00439      False    False     False  False  \n",
       "2018-Es-04919      False    False      True  False  \n",
       "2018-Es-02703      False    False     False  False  \n",
       "2018-Es-02680      False     True     False  False  \n",
       "\n",
       "[3561 rows x 12 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sem_eval_train_es.csv', index_col='ID')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_md')\n",
    "\n",
    "pattern2 = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "\n",
    "def clean_text(text, lemas=False):\n",
    "    text = re.sub(r'@[\\w_]+|https?://[\\w_./]+', '', text)\n",
    "    tokens = nlp(text)\n",
    "    tokens = [tok.lemma_.lower() if lemas else tok.lower_ for tok in tokens if not tok.is_punct]\n",
    "    filtered_tokens = [pattern2.sub('', tok) for tok in tokens]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Tweet = df.Tweet.apply(clean_text, lemas = True)\n",
    "df[df['Tweet'] != '']\n",
    "\n",
    "label_columns = ['anger', 'anticipation', 'disgust', 'fear', 'joy', \n",
    "                 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n",
    "df[label_columns] = df[label_columns].astype(int)\n",
    "Y = df[label_columns].values\n",
    "\n",
    "# Separar los datos en entrenamiento y prueba\n",
    "tweets_train, tweets_test, Y_train, Y_test = train_test_split(df.Tweet, Y, \n",
    "    test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2003"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(min_df=2)\n",
    "cv.fit(tweets_train)\n",
    "max_features = len(cv.get_feature_names_out())\n",
    "max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6378 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = max_features+2\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, split = ' ', oov_token='OOV')\n",
    "tokenizer.fit_on_texts(tweets_train.values)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(tweets_train.values)\n",
    "X_train = pad_sequences(X_train, padding='post')\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens.')\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = X_train.shape[1]\n",
    "X_test = tokenizer.texts_to_sequences(tweets_test.values)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_md')\n",
    "\n",
    "EMBREDDING_DIM = nlp.vocab.vectors_length\n",
    "embedding_matrix = np.zeros((MAX_NB_WORDS, EMBREDDING_DIM))\n",
    "vectores = 0\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if(i<MAX_NB_WORDS):\n",
    "        if nlp.vocab[word].has_vector:\n",
    "            embedding_matrix[i] = nlp.vocab[word].vector\n",
    "            vectores += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 32, 300)           601500    \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 32, 300)          0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 50)                70200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 11)                561       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 672,261\n",
      "Trainable params: 70,761\n",
      "Non-trainable params: 601,500\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 19:41:34.390250: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-05 19:41:34.390404: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-09-05 19:41:34.390459: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2024-09-05 19:41:34.390496: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2024-09-05 19:41:34.390530: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2024-09-05 19:41:34.390565: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2024-09-05 19:41:34.390598: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2024-09-05 19:41:34.390632: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2024-09-05 19:41:34.390659: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2024-09-05 19:41:34.390663: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-09-05 19:41:34.390948: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(MAX_NB_WORDS,\n",
    "                            EMBREDDING_DIM,\n",
    "                            weights = [embedding_matrix],\n",
    "                            input_length = MAX_SEQUENCE_LENGTH,\n",
    "                            trainable = False,\n",
    "                            mask_zero = True)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(RNN_DIM, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(11, activation='sigmoid'))\n",
    "model.compile(loss='binry_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trabajonlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
